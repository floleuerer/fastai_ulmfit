{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained\n",
    "\n",
    "> fast.ai ULMFiT helpers to easily use pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import json\n",
    "from fastai.text.all import SentencePieceTokenizer, SpacyTokenizer, language_model_learner, \\\n",
    "                            text_classifier_learner, untar_data, Path, patch, \\\n",
    "                            LMLearner, os, pickle, shutil, AWD_LSTM, accuracy, \\\n",
    "                            Perplexity, delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_config(path):\n",
    "    with open(path/'model.json', 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_pretrained_model(url):\n",
    "    fname = f\"{url.split('/')[-1]}.tgz\"\n",
    "    path = untar_data(url, fname=fname, c_key='model')\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_direction(backwards):\n",
    "    return 'bwd' if backwards else 'fwd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_class(classname):\n",
    "    cls = None\n",
    "    if len(classname.split('.')) > 1:\n",
    "        comp = classname.rsplit('.', 1)\n",
    "        imported = import_module(comp[0])\n",
    "        cls = getattr(imported, comp[1])\n",
    "    else:\n",
    "        cls = globals()[classname]\n",
    "    return cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "assert(_get_direction(backwards=False) == 'fwd')\n",
    "assert(_get_direction(backwards=True) == 'bwd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `model` and `vocab` files from path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_files(path, backwards=False, encoder=False):\n",
    "    direction = _get_direction(backwards)\n",
    "    #config = _get_config(path/direction)\n",
    "    try: \n",
    "        model_path = path/direction\n",
    "        if encoder:\n",
    "            model_file = list(model_path.glob(f'*encoder.pth'))[0]\n",
    "        else:\n",
    "            model_file = list(model_path.glob(f'*model.pth'))[0]\n",
    "            \n",
    "        vocab_file = list(model_path.glob(f'*vocab.pkl'))[0]\n",
    "        fnames = [model_file.absolute(),vocab_file.absolute()]\n",
    "    except IndexError: print(f'The model in {model_path} is incomplete, download again'); raise\n",
    "    fnames = [str(f.parent/f.stem) for f in fnames]\n",
    "    return fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get `tokenizer` from model-config. Tokenizer parameters in `model.json` will be passed to the Tokenizer. As of now SentencePiece and Spacy are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenizer_from_pretrained(url, pretrained=False, backwards=False, **kwargs):\n",
    "    path = _get_pretrained_model(url)\n",
    "    direction = _get_direction(backwards)\n",
    "    config = _get_config(path/direction)\n",
    "    \n",
    "    if config['tokenizer']['class'] == 'SentencePieceTokenizer':\n",
    "        if pretrained: config['tokenizer']['params']['sp_model'] = path/'spm'/'spm.model'\n",
    "        \n",
    "    tok_cls = _get_class(config['tokenizer']['class'])\n",
    "    tok = tok_cls(**config['tokenizer']['params'])\n",
    "    \n",
    "    return tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `langauge_model_learner` from pretrained model-URL. All parameters will be passed to `language_model_learner`. The following parameters are set automatically: `arch`, `pretrained` and `pretrained_fnames`. By default `accuracy` and `perplexity` are passed as `metrics`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(language_model_learner)\n",
    "def language_model_from_pretrained(dls, url=None, backwards=False, metrics=None, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config\n",
    "    path = _get_pretrained_model(url)\n",
    "    fnames = _get_model_files(path, backwards=backwards)\n",
    "    metrics = [accuracy, Perplexity()] if metrics == None else metrics\n",
    "    return language_model_learner(dls, \n",
    "                                  arch, \n",
    "                                  pretrained=True, \n",
    "                                  pretrained_fnames=fnames, \n",
    "                                  metrics=metrics,\n",
    "                                  **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _get_model_path(learn=None, path=None):\n",
    "    path = (learn.path/learn.model_dir) if not path else Path(path)\n",
    "    if not path.exists(): os.makedirs(path, exist_ok=True)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves the following model files to `path`:\n",
    "- Model (`lm_model.pth`)\n",
    "- Encoder (`lm_encoder.pth`)\n",
    "- Vocab from dataloaders (`lm_vocab.pkl`)\n",
    "- SentencePieceModel (`spm/`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def save_lm(x:LMLearner, path=None, with_encoder=True, backwards=False):\n",
    "    path = _get_model_path(x, path)\n",
    "    direction = _get_direction(backwards)\n",
    "    model_path = path/direction\n",
    "    if not model_path.exists(): os.makedirs(model_path, exist_ok=True)\n",
    "        \n",
    "    x.to_fp32()\n",
    "    # save model\n",
    "    x.save((model_path/'lm_model').absolute(), with_opt=False)\n",
    "    \n",
    "    # save encoder\n",
    "    if with_encoder:\n",
    "        x.save_encoder((model_path/'lm_encoder').absolute())\n",
    "\n",
    "    # save vocab\n",
    "    with open((model_path/'lm_vocab.pkl').absolute(), 'wb') as f:\n",
    "        pickle.dump(x.dls.vocab, f)\n",
    "       \n",
    "    # save tokenizer if SentencePiece is used\n",
    "    if isinstance(x.dls.tok, SentencePieceTokenizer):\n",
    "        # copy SPM if path not spm path\n",
    "        spm_path = Path(x.dls.tok.cache_dir)\n",
    "        if path.absolute() != spm_path.absolute():\n",
    "            target_path = path/'spm'\n",
    "            if not target_path.exists(): os.makedirs(target_path, exist_ok=True)\n",
    "            shutil.copyfile(spm_path/'spm.model', target_path/'spm.model')\n",
    "            shutil.copyfile(spm_path/'spm.vocab', target_path/'spm.vocab')\n",
    "    \n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def vocab_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)\n",
    "#    with open((path/'lm_vocab.pkl').absolute(), 'rb') as f:\n",
    "#        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def spm_from_lm(learn=None, path=None):\n",
    "#    path = _get_model_path(learn, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `text_classifier_learner` from fine-tuned model path (saved with `learn.save_lm()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(text_classifier_learner)\n",
    "def text_classifier_from_lm(dls, path=None, backwards=False, **kwargs):\n",
    "    arch = AWD_LSTM # TODO: Read from config / _get_class()\n",
    "    path = _get_model_path(path=path)\n",
    "    fnames = _get_model_files(path, backwards=backwards, encoder=True)\n",
    "    learn = text_classifier_learner(dls, arch, pretrained=False, **kwargs)\n",
    "    learn.load_encoder(fnames[0])\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests - Tokenizer, LM and Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "url = 'http://localhost:8080/ulmfit-dewiki.tgz'\n",
    "tok = tokenizer_from_pretrained(url, pretrained=True)\n",
    "assert(tok.vocab_sz == 15000)\n",
    "assert('ulmfit-dewiki/spm/spm.model' in str(tok.sp_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#slow\n",
    "tok = tokenizer_from_pretrained(url, pretrained=False)\n",
    "assert(tok.sp_model == None)\n",
    "assert(tok.vocab_sz == 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/fastai-ulmfit/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6.461167</td>\n",
       "      <td>6.521255</td>\n",
       "      <td>0.170177</td>\n",
       "      <td>679.430847</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_lm_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, is_lm=True, backwards=backwards)),\n",
    "                    get_x=ColReader('text'), \n",
    "                    splitter=RandomSplitter(valid_pct=0.1, seed=42))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = language_model_from_pretrained(dls, url=url, backwards=backwards)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Path('models')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "path = learn.save_lm()\n",
    "vocab = learn.dls.vocab\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/miniconda3/envs/fastai-ulmfit/lib/python3.8/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.744500</td>\n",
       "      <td>0.689982</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5193, 0.4807],\n",
       "         [0.4815, 0.5185],\n",
       "         [0.5388, 0.4612],\n",
       "         [0.5185, 0.4815],\n",
       "         [0.5308, 0.4692],\n",
       "         [0.5061, 0.4939],\n",
       "         [0.5493, 0.4507],\n",
       "         [0.5111, 0.4889],\n",
       "         [0.5562, 0.4438],\n",
       "         [0.5348, 0.4652],\n",
       "         [0.5624, 0.4376],\n",
       "         [0.4860, 0.5140],\n",
       "         [0.4883, 0.5117],\n",
       "         [0.4892, 0.5108],\n",
       "         [0.4824, 0.5176],\n",
       "         [0.5322, 0.4678],\n",
       "         [0.5151, 0.4849],\n",
       "         [0.4987, 0.5013],\n",
       "         [0.4727, 0.5273],\n",
       "         [0.5062, 0.4938],\n",
       "         [0.4856, 0.5144],\n",
       "         [0.5231, 0.4769],\n",
       "         [0.4989, 0.5011],\n",
       "         [0.5040, 0.4960],\n",
       "         [0.4821, 0.5179],\n",
       "         [0.5580, 0.4420],\n",
       "         [0.5089, 0.4911],\n",
       "         [0.5447, 0.4553],\n",
       "         [0.5216, 0.4784],\n",
       "         [0.4987, 0.5013],\n",
       "         [0.5668, 0.4332],\n",
       "         [0.4845, 0.5155],\n",
       "         [0.4946, 0.5054],\n",
       "         [0.4987, 0.5013],\n",
       "         [0.4946, 0.5054],\n",
       "         [0.5385, 0.4615],\n",
       "         [0.5198, 0.4802],\n",
       "         [0.5629, 0.4371],\n",
       "         [0.4950, 0.5050],\n",
       "         [0.5415, 0.4585],\n",
       "         [0.4716, 0.5284],\n",
       "         [0.5027, 0.4973],\n",
       "         [0.5433, 0.4567],\n",
       "         [0.5420, 0.4580],\n",
       "         [0.4830, 0.5170],\n",
       "         [0.5097, 0.4903],\n",
       "         [0.4666, 0.5334],\n",
       "         [0.5343, 0.4657],\n",
       "         [0.4993, 0.5007],\n",
       "         [0.4992, 0.5008],\n",
       "         [0.4925, 0.5075],\n",
       "         [0.5200, 0.4800],\n",
       "         [0.4952, 0.5048],\n",
       "         [0.4924, 0.5076],\n",
       "         [0.5507, 0.4493],\n",
       "         [0.5216, 0.4784],\n",
       "         [0.4877, 0.5123],\n",
       "         [0.5105, 0.4895],\n",
       "         [0.4884, 0.5116],\n",
       "         [0.5515, 0.4485],\n",
       "         [0.4584, 0.5416],\n",
       "         [0.4969, 0.5031],\n",
       "         [0.5194, 0.4806],\n",
       "         [0.5771, 0.4229],\n",
       "         [0.4928, 0.5072],\n",
       "         [0.5024, 0.4976],\n",
       "         [0.4903, 0.5097],\n",
       "         [0.5168, 0.4832],\n",
       "         [0.5055, 0.4945],\n",
       "         [0.5544, 0.4456],\n",
       "         [0.5195, 0.4805],\n",
       "         [0.4895, 0.5105],\n",
       "         [0.4891, 0.5109],\n",
       "         [0.5118, 0.4882],\n",
       "         [0.5241, 0.4759],\n",
       "         [0.4970, 0.5030],\n",
       "         [0.4850, 0.5150],\n",
       "         [0.4947, 0.5053],\n",
       "         [0.4979, 0.5021],\n",
       "         [0.5492, 0.4508],\n",
       "         [0.5057, 0.4943],\n",
       "         [0.5123, 0.4877],\n",
       "         [0.5140, 0.4860],\n",
       "         [0.5226, 0.4774],\n",
       "         [0.4931, 0.5069],\n",
       "         [0.4816, 0.5184],\n",
       "         [0.5125, 0.4875],\n",
       "         [0.4673, 0.5327],\n",
       "         [0.5094, 0.4906],\n",
       "         [0.5226, 0.4774],\n",
       "         [0.4641, 0.5359],\n",
       "         [0.5127, 0.4873],\n",
       "         [0.4921, 0.5079],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.4961, 0.5039],\n",
       "         [0.4825, 0.5175],\n",
       "         [0.5333, 0.4667],\n",
       "         [0.5394, 0.4606],\n",
       "         [0.4703, 0.5297],\n",
       "         [0.5192, 0.4808],\n",
       "         [0.5010, 0.4990],\n",
       "         [0.4846, 0.5154],\n",
       "         [0.5201, 0.4799],\n",
       "         [0.5177, 0.4823],\n",
       "         [0.5153, 0.4847],\n",
       "         [0.5240, 0.4760],\n",
       "         [0.5055, 0.4945],\n",
       "         [0.5428, 0.4572],\n",
       "         [0.4890, 0.5110],\n",
       "         [0.4969, 0.5031],\n",
       "         [0.4699, 0.5301],\n",
       "         [0.5085, 0.4915],\n",
       "         [0.4889, 0.5111],\n",
       "         [0.5114, 0.4886],\n",
       "         [0.4977, 0.5023],\n",
       "         [0.5179, 0.4821],\n",
       "         [0.5005, 0.4995],\n",
       "         [0.5203, 0.4797],\n",
       "         [0.5514, 0.4486],\n",
       "         [0.5142, 0.4858],\n",
       "         [0.4991, 0.5009],\n",
       "         [0.5509, 0.4491],\n",
       "         [0.5053, 0.4947],\n",
       "         [0.5498, 0.4502],\n",
       "         [0.4732, 0.5268],\n",
       "         [0.5479, 0.4521],\n",
       "         [0.5465, 0.4535],\n",
       "         [0.5146, 0.4854],\n",
       "         [0.4941, 0.5059],\n",
       "         [0.5232, 0.4768],\n",
       "         [0.5344, 0.4656],\n",
       "         [0.5301, 0.4699],\n",
       "         [0.5294, 0.4706],\n",
       "         [0.5234, 0.4766],\n",
       "         [0.4713, 0.5287],\n",
       "         [0.5081, 0.4919],\n",
       "         [0.5292, 0.4708],\n",
       "         [0.5257, 0.4743],\n",
       "         [0.4831, 0.5169],\n",
       "         [0.5107, 0.4893],\n",
       "         [0.4765, 0.5235],\n",
       "         [0.5158, 0.4842],\n",
       "         [0.5112, 0.4888],\n",
       "         [0.4855, 0.5145],\n",
       "         [0.4961, 0.5039],\n",
       "         [0.4789, 0.5211],\n",
       "         [0.5201, 0.4799],\n",
       "         [0.5617, 0.4383],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.4901, 0.5099],\n",
       "         [0.5142, 0.4858],\n",
       "         [0.5071, 0.4929],\n",
       "         [0.4929, 0.5071],\n",
       "         [0.4786, 0.5214],\n",
       "         [0.4968, 0.5032],\n",
       "         [0.5464, 0.4536],\n",
       "         [0.5297, 0.4703],\n",
       "         [0.5339, 0.4661],\n",
       "         [0.5408, 0.4592],\n",
       "         [0.4880, 0.5120],\n",
       "         [0.4914, 0.5086],\n",
       "         [0.5222, 0.4778],\n",
       "         [0.4808, 0.5192],\n",
       "         [0.4738, 0.5262],\n",
       "         [0.4784, 0.5216],\n",
       "         [0.5005, 0.4995],\n",
       "         [0.5323, 0.4677],\n",
       "         [0.4658, 0.5342],\n",
       "         [0.5014, 0.4986],\n",
       "         [0.4915, 0.5085],\n",
       "         [0.5208, 0.4792],\n",
       "         [0.4796, 0.5204],\n",
       "         [0.4799, 0.5201],\n",
       "         [0.5056, 0.4944],\n",
       "         [0.5243, 0.4757],\n",
       "         [0.5238, 0.4762],\n",
       "         [0.5028, 0.4972],\n",
       "         [0.5101, 0.4899],\n",
       "         [0.5466, 0.4534],\n",
       "         [0.5093, 0.4907],\n",
       "         [0.5516, 0.4484],\n",
       "         [0.5341, 0.4659],\n",
       "         [0.5463, 0.4537],\n",
       "         [0.5212, 0.4788],\n",
       "         [0.5015, 0.4985],\n",
       "         [0.5112, 0.4888],\n",
       "         [0.5048, 0.4952],\n",
       "         [0.5066, 0.4934],\n",
       "         [0.4739, 0.5261],\n",
       "         [0.4954, 0.5046],\n",
       "         [0.4966, 0.5034],\n",
       "         [0.4778, 0.5222],\n",
       "         [0.5288, 0.4712],\n",
       "         [0.4870, 0.5130],\n",
       "         [0.5080, 0.4920],\n",
       "         [0.5215, 0.4785],\n",
       "         [0.4571, 0.5429],\n",
       "         [0.5093, 0.4907],\n",
       "         [0.5169, 0.4831],\n",
       "         [0.5472, 0.4528],\n",
       "         [0.4801, 0.5199],\n",
       "         [0.4930, 0.5070],\n",
       "         [0.4929, 0.5071],\n",
       "         [0.5035, 0.4965]]),\n",
       " TensorCategory([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "         1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "         0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "#slow\n",
    "from fastai.text.all import AWD_LSTM, DataBlock, TextBlock, ColReader, RandomSplitter, CategoryBlock\n",
    "import pandas as pd\n",
    "\n",
    "backwards = False\n",
    "\n",
    "df = pd.read_csv(Path('_test/data_class_sample.csv'))\n",
    "\n",
    "dblocks = DataBlock(blocks=(TextBlock.from_df('text', tok=tok, vocab=vocab, backwards=backwards), CategoryBlock),\n",
    "                    get_x=ColReader('text'), \n",
    "                    get_y=ColReader('label'))\n",
    "dls = dblocks.dataloaders(df, bs=128)\n",
    "\n",
    "learn = text_classifier_from_lm(dls, path=path, backwards=backwards)\n",
    "learn.fit_one_cycle(1)\n",
    "learn.get_preds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai-ulmfit",
   "language": "python",
   "name": "fastai-ulmfit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
